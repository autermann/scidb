<?xml version="1.0" encoding="UTF-8"?>
<!-- This document was created with Syntext Serna Free. -->
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"docbookV4.5/docbookx.dtd">
<chapter>
  <title>Loading Data</title>

  <para>A key part of setting up your SciDB array is loading your data. SciDB
  supports several load techniques, which together accommodate a wide range of
  scenarios for moving data into SciDB.</para>

  <para>This chapter first presents the overview of loading data into SciDB:
  the basic principles and general steps that apply to all load techniques.
  Then it describes each load technique in turn. Finally this chapter presents
  some detailed information (such as handling errors during load) that applies
  to all the techniques.</para>

  <sect1>
    <title>Overview of Moving Data Into SciDB</title>

    <para>You typically load data into SciDB one array at a time. In most
    situations, if you need three arrays, you will perform three separate
    loads. Regardless of the specific data-loading technique you use, the
    general steps for moving data into SciDB are as follows:</para>

    <orderedlist>
      <listitem>
        <para>Visualize the shape of the data as you want it to appear in a
        SciDB array.</para>

        <para>Remember, the goal of loading data into SciDB is to make it
        available for array processing. Before you load the data, you should
        assess your analytical needs to determine what arrays you will need.
        You also must determine for each array what variables it will include
        and which of those variables will be dimensions and which will be
        attributes. For more information about creating arrays, see Creating
        and Removing SciDB Arrays.</para>

        <para>Depending on the data-loading technique you choose, this
        preliminary assessment might or might not include determining chunk
        sizes and chunk overlaps.</para>
      </listitem>

      <listitem>
        <para>Prepare the data files for loading into SciDB.</para>

        <para>Depending on the specific technique you are using, this can mean
        creating a binary file, a single file in SciDB format, or a CSV
        file.</para>
      </listitem>

      <listitem>
        <para>Load the data into SciDB.</para>

        <para>In all cases, this will mean invoking the LOAD command, either
        explicitly or indirectly through the loadcsv.py shell command.
        Different techniques may require different command options and
        syntax.</para>
      </listitem>

      <listitem>
        <para>Rearrange the loaded data into the target array; the
        multi-dimensional array that supports your analytics.</para>

        <para>For some loading techniques, such rearrangement will typically
        involve the redimension_store operator and possibly involve the
        analyze operator. The program loadcsv.py, which is the linchpin of the
        parallel load technique, can perform this step for you.</para>
      </listitem>
    </orderedlist>
  </sect1>

  <sect1>
    <title>Loading CSV Data</title>

    <para>The CSV loading technique starts from a file in
    comma-separated-value (CSV) format, translates it into a SciDB-formatted
    text file describing a one-dimensional array, loads that file into a
    1-dimensional array in SciDB, and rearranges that 1-dimensional array into
    the multi-dimensional shape you need to support your querying and
    analytics. The following figure presents an overview:</para>

    <figure>
      <title/>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentdepth="300" depth="300"
                     fileref="../graphics/csv_load_overview.png" scale="30"
                     valign="top" width="400"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>Obviously, the CSV loading technique commends itself to situations
    in which your external application produces a CSV file. If you have a CSV
    file, you will use either the CSV loading technique or the parallel
    loading technique described elsewhere in this chapter. But if you can
    control the format that the external application uses to produce the data,
    you might choose to produce a CSV file and to use CSV loading technique in
    the following situations:</para>

    <itemizedlist>
      <listitem>
        <para>For loading small arrays, such as arrays that will be lookup
        arrays or utility arrays that will be combined with other, larger
        arrays.</para>
      </listitem>

      <listitem>
        <para>For loading data into an intermediate SciDB array before you
        have determined the chunk sizes for the dimensions of the target
        array.</para>
      </listitem>
    </itemizedlist>

    <sect2>
      <title>Visualize the Target Array</title>

      <para>When using the CSV loading technique, visualizing the desired
      SciDB array means the following:</para>

      <itemizedlist>
        <listitem>
          <para>Determine the attributes for the array, including each
          attribute's name, datatype, whether it allows null values, and
          whether it has a default value to be used to replace null
          values.</para>
        </listitem>

        <listitem>
          <para>Determine the dimensions of the target array, including each
          dimension's name and datatype.</para>
        </listitem>
      </itemizedlist>

      <para>When using the CSV load technique, you can postpone contemplating
      each dimension's chunk size until after you have loaded the data into
      the intermediate 1-D array. This lets you use the analyze operator on
      that array to learn some simple statistics about the loaded data that
      can help you choose chunk sizes and chunk overlaps for each dimension of
      the target array.</para>

      <para>For example, suppose you want an array with two dimensions and two
      attributes, like this:</para>

      <figure>
        <title/>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="500" depth="200"
                       fileref="../graphics/desired_array_olympics.png"
                       scale="50" valign="top" width="600"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The dimensions are "year" and "event." The attributes are "person"
      and "time." The top right cell indicates, for example, that in 2008 Bolt
      won the dash in 9.69 seconds.</para>

      <para>This simple, 12-cell array will be the target array used to
      illustrate steps of the CSV load technique.</para>
    </sect2>

    <sect2>
      <title>Prepare the Load File</title>

      <para>The CSV loading technique starts with a comma-separated-value
      (csv) file. Each row of the file describes one cell of the target array,
      including its dimension values. Because the target array has four
      variables (two dimensions and two attributes), each row of the csv file
      will have four values. Because the target array has twelve non-empty
      cells, the csv file will have 12 rows of data, like this:</para>

      <programlisting language="test">
--shell --show-query=no --command=cat ../examples/olympic_data.csv
</programlisting>

      <para>To include a null value for an attribute, you have several
      choices:</para>

      <itemizedlist>
        <listitem>
          <para>Leave the field empty: no space, no tab, no ASCII character
          whatsoever.</para>
        </listitem>

        <listitem>
          <para>Use a question mark--with nothing else--in place of the value.
          (By contrast, if the value you want to load is the question mark
          character itself, put it in quotation marks.</para>
        </listitem>

        <listitem>
          <para>Use a question mark immediately followed by an integer between
          0 and 127 (inclusive). The integer you use is the "missing reason
          code" for the null value.</para>
        </listitem>
      </itemizedlist>

      <para>For more information about null processing, see the section on
      Basic Architecture in Introduction to SciDB.</para>

      <para>After you create the csv file, you must convert it to the SciDB
      dense load format. For that, use the <code>csv2scidb</code> shell
      command. The <code>csv2scidb</code> command takes multicolumn csv data
      and transforms it into a format that the SciDB loader will recognize as
      a 1-dimensional array with one attribute for every column of the
      original csv file. The syntax of <code>csv2scidb</code> is:
      <programlisting>csv2scidb [options]  &lt; input-file  &gt; output-file </programlisting></para>

      <note>
        <para><code>csv2scidb</code> is accessed directly at the command-line
        and not through the <code>iquery</code> client.</para>
      </note>

      <para>To see the options for <code>csv2scidb</code>, type
      <code>csv2scidb --help</code> at the command line. The options for
      csv2scidb are: <programlisting language="test">
--shell --show-query=no --show-output=yes --command=csv2scidb -h
</programlisting>This command will transform <code>olympic_data.csv</code> to
      SciDB load file format: <programlisting>
csv2scidb -s 1 -p SNSN &lt; .../examples/olympic_data.csv &gt; .../examples/olympic_data.scidb
</programlisting>The -s flag specifies the number of lines to skip at the
      beginning of the file. Since the file has a header, you can strip that
      line with "-s 1". The -p flag specifies the types of data in the columns
      you are transforming. Possible values are N (number), S (string), s
      (nullable string), and C (char).</para>

      <para>The file <code>olympic_data.scidb</code> looks like this:
      <programlisting language="test">
--shell --show-output=yes --command=cat ../examples/olympic_data.scidb
</programlisting>The square braces show the beginning and end of the array
      dimension. The parentheses enclose the individual cells of the array.
      There are commas between attributes in cells and between cells in the
      array.</para>
    </sect2>

    <sect2>
      <title>Load the Data</title>

      <para>After you prepare the file in the SciDB dense-load format, you are
      almost ready to load the data into SciDB. But first you must create an
      array to serve as the load array. The array must have one dimension and
      N attributes, where N is the number of columns in the original csv file.
      For the olympic data, the array that you create must have four
      attributes, like this:</para>

      <para><programlisting language="test">
--aql CREATE ARRAY winnersFlat &lt; event:string, year:int64, person:string, time:double &gt; [i=0:*,1000000,0]; --show-output=no 
</programlisting></para>

      <para>Within the preceding CREATE ARRAY statement, notice the
      following:</para>

      <itemizedlist>
        <listitem>
          <para>The attribute names -- Even if you plan to delete the
          1-dimensional load array after you create the target 2-dimensional,
          2-attribute array--the attribute names matter. You should name the
          attributes as you expect to name the corresponding attribute and
          dimensions in the array you will ultimately create to support your
          analytics.</para>
        </listitem>

        <listitem>
          <para>The order of attributes -- You must declare the attributes in
          the same left-to-right order as the values that appear on each line
          of the csv file.</para>
        </listitem>

        <listitem>
          <para>The dimension name -- The dimension name ("i" in this case) is
          uninteresting. You can use any name, because that dimension does not
          correspond to any variable from your data set and that dimension
          will not appear in any form in the target array. Remember that
          within the load array, every variable of your data appears as an
          attribute. These variables are not rearranged into attributes and
          dimensions until the last step of the procedure. (Although the
          dimension name is uninteresting, its values will correspond to the
          corresponding row number in the CSV file.)</para>
        </listitem>

        <listitem>
          <para>The chunk size (in this case, 1000000) for the dimension --
          Even though you are likely to use the winnersFlat array only briefly
          and perhaps delete it after you populate the target array, the chunk
          size matters because it can affect performance of the load and of
          the next step: the redimension_store.</para>
        </listitem>

        <listitem>
          <para>The chunk overlap (in this case, 0) for the dimension. If you
          are using the load array briefly--only as the target of the load
          operation and as the source of the subsequent redimension-store
          operation--then there is no need for chunks in the load array to
          overlap at all.</para>
        </listitem>
      </itemizedlist>

      <para>For more information about chunk size and overlap, see the section
      on Basic Architecture in Introduction to SciDB.</para>

      <para>After you create the target array, you can populate it with data
      using the <code>LOAD</code> statement: <programlisting language="test">
--aql LOAD winnersFlat FROM '../examples/olympic_data.scidb'; --show-output=no
</programlisting>The data file paths in the AFL and AQL commands are relative
      to the working directory of the server.</para>
    </sect2>

    <sect2>
      <title>Rearrange As Necessary</title>

      <para>After you establish the load array, you can use SciDB features to
      translate it into the target array whose shape accommodates your
      analytical needs. Of course, you should have the basic shape of the
      target array in mind from the outset -- perhaps even before you created
      the csv file.</para>

      <para>There are, however, some characteristics of arrays beyond these
      basics. These include the chunk size and chunk overlap value of each
      dimension. Before you choose values for these parameters, you can use
      the SciDB analyze operator to learn some simple statistics about the
      data in the array you just loaded. Here is the command to analyze the
      array winnersFlat:</para>

      <programlisting language="test">
--aql SELECT * FROM analyze(winnersFlat);
</programlisting>

      <para>For the simple example presented here, the simple statistics
      reveal little of interest. For large arrays however, the data can be
      illuminating and can influence your decisions about chunk size and chunk
      overlap. For more information about chunk size and overlap, see the
      section on Basic Architecture in Introduction to SciDB. For more
      information about the analyze operator, see the Analyze section in SciDB
      Operator Reference.</para>

      <para>The following command creates the target array:</para>

      <programlisting language="test">
--aql CREATE ARRAY winners &lt;person:string, time:double&gt; 
[year=1996:2008,1000,0, event(string)=3,1000,0] --show-output=no 
</programlisting>

      <para>The result of that command is an array that can accommodate the
      data about olympic winners. To populate this array with the data, use
      the following command:</para>

      <programlisting language="test">
--aql SELECT * FROM redimension_store(winnersFlat,winners); --show-output=no 
</programlisting>

      <para>The result of this command is the desired array; you have
      completed the CSV load procedure.</para>
    </sect2>
  </sect1>

  <sect1>
    <title>Parallel Load</title>

    <para>Like the CSV load technique, the parallel technique starts from a
    single CSV file. However, there are significant differences that can yield
    much faster load performance. Parallel load separates the CSV file into
    multiple files and distributes those files among the server instances in
    your SciDB cluster, allowing those instances to work in parallel on the
    load. In addition, parallel load can transfer data through pipes (rather
    than through materialized intermediate files), which also improves
    performance.</para>

    <para>The following figure presents an overview:</para>

    <figure>
      <title/>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentdepth="500" depth="500"
                     fileref="../graphics/parallel_load_overview_simplified.png"
                     scale="30" valign="top" width="500"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>In the picture, notice the following:</para>

    <orderedlist>
      <listitem>
        <para>The program loadcsv.py performs a number of steps, starting with
        partitioning the original CSV file into k distinct subsets, where k is
        the number of SciDB instances in the cluster.</para>
      </listitem>

      <listitem>
        <para>The k subsets of the original CSV file can be files or pipes.
        Pipes are faster, but you can use files to troubleshoot your load
        processes.</para>
      </listitem>

      <listitem>
        <para>The program loadcsv.py converts each of the individual CSV files
        into data that conforms to the SciDB dense file format. Here too, the
        data can be expressed as files or pipes.</para>
      </listitem>

      <listitem>
        <para>The program invokes the SciDB load k times, once for each
        dense-load-format file. Each of these load operations runs on a
        separate SciDB instance in your cluster.</para>
      </listitem>

      <listitem>
        <para>The resulting 1-dimensional array is equivalent to the load
        array that would be produced by the (non-parallel) CSV load
        technique.</para>
      </listitem>

      <listitem>
        <para>You can run redimension_store explicitly, or you can instruct
        loadcsv.py to do it for you with the -A command line switch.</para>
      </listitem>
    </orderedlist>

    <para>The parallel loading technique is recommended for situations in
    which your external application produces a very large CSV file.</para>

    <sect2>
      <title>Visualize the Target Array</title>

      <para>When using the parallel loading technique, visualizing the target
      SciDB array means the following:</para>

      <itemizedlist>
        <listitem>
          <para>Determine the attributes for the array, including each
          attribute's name, datatype, whether it allows null values, and
          whether it has a default value to be used to replace null
          values.</para>
        </listitem>

        <listitem>
          <para>Determine the dimensions of the array, including each
          dimension's name and datatype.</para>
        </listitem>
      </itemizedlist>

      <para>As with the CSV load technique, you can postpone contemplating
      each dimension's chunk size until after you have loaded the data into
      the intermediate 1-D array. This lets you use the analyze operator on
      that array to learn some simple statistics about the loaded data that
      can help you choose chunk sizes and chunk overlaps for each dimension of
      the multi-dimensional array you desire.</para>
    </sect2>

    <sect2>
      <title>Load the Data</title>

      <para>The linchpin of the parallel load technique is the program
      loadcsv.py. Its primary input is a single CSV file and its primary
      result is a 1-D SciDB array: the load array. Besides its primary input,
      you can specify additional parameters to the program with command-line
      switches. Likewise, you can use switches to control the by-products of
      the parallel load operation.</para>

      <para>The syntax of <code>loadcsv.py</code> is: <programlisting>loadcsv.py [options]  </programlisting></para>

      <note>
        <para><code>loadcsv.py</code> is accessed directly at the command-line
        and not through the <code>iquery</code> client. Furthermore, you must
        run loadcsv.py from the SciDB admin account.</para>
      </note>

      <para>To see the options for <code>loadcsv.py</code>, type
      <code>loadcsv.py --help</code> at the command line. The options for
      loadcsv.py are: <programlisting language="test">
--shell --show-output=yes --show-query=no --command=loadcsv.py -h
</programlisting></para>

      <para>The command-line switches work in combination to control these
      aspects of the parallel load process:</para>

      <itemizedlist>
        <listitem>
          <para>The operation of csv2scidb</para>

          <para>Remember, loadcsv.py invokes csv2scidb, a utility that itself
          requires some switches. On the loadcsv.py command line, you use the
          -i switch to indicate the location of the input CSV file, -n to
          indicate the number of lines at the top of the CSV file to be
          skipped, -t to indicate the CSV field-type pattern, -f to indicate
          the starting dimension index, and -D to indicate the character
          delimiter.</para>
        </listitem>

        <listitem>
          <para>Location of SciDB instance, its data directory, and its
          attendant utilities.</para>

          <para>The loadcsv.py program needs to know details about the SciDB
          installation. You supply these details with -d, which indicates the
          hostname or IP address of the coodinator instance, -p, which
          indicates the port number on which the coordinator instance is
          listening, and -r, which indicates the root installation folder
          containing the utilities csv2scidb, iquery, and splitcsv. (The
          utility splitcsv partitions the input CSV file into separate files
          to be distributed among the SciDB instances for parallel
          loading.)</para>
        </listitem>

        <listitem>
          <para>SSH connectivity</para>

          <para>The loadcsv.py program connects to the SciDB cluster through
          SSH. The program requires that each node in the cluster has SSH
          configured on the same port; use -P to indicate that port. Use -u to
          indicate the SSH username. Use -k to supply the SSH Key/Identify
          file used to authenticate the the SSH user on the remote node.
          (loadcsv.py requires that password-less SSH is configured for everny
          node in the cluster.)</para>

          <para>In certain situations, SSH authentication presents a
          confirmation step in the user interface. Use the -b switch to bypass
          this step. If you do not want to use -b (because it weakens
          security), you can instead manually connect through SSH to each node
          in the cluster before you use loadcsv.py.</para>
        </listitem>

        <listitem>
          <para>Characteristics and handling of the 1-dimensional load
          array.</para>

          <para>The loadcsv.py program can operate on an existing
          1-dimensional array, create a new one, or even delete an existing
          one before creating a new one. You control this behavior with the
          switches -c, -a, -s, and -x. The switch -c controls the chunk size
          of the load array. Use -a to supply the name of the array. Use -s to
          supply a schema definition if you want loadcsv.py to create the load
          array for you. Use -x to empower loadcsv.py to delete any existing
          array before creating the new one you described with the -a and -s
          switches. The -x switch is a safeguard to ensure that you do not
          inadvertantly delete a 1-dimensional array that you need. The -x
          switch is meaningless if you do not supply both -a and -s.</para>
        </listitem>

        <listitem>
          <para>Characteristics and handling of the multi-dimensional target
          array.</para>

          <para>The loadcsv.py program can populate a multidimensional array
          to support your analytics. It can populate an existing array, create
          and populate a new one, or even delete an existing one before
          creating and populating a new one. You control this behavior with
          the switches -A, -S, and -X. Use -A to supply the name of the array.
          Use -S to supply a schema definition if you want loadcsv.py to
          create the target array for you. Use -x to empower loadcsv.py to
          delete any existing array before creating the new one you described
          with the -A and -S switches. The -X switch is a safeguard to ensure
          that you do not inadvertantly delete a multi-dimensional array that
          you need. The -X switch is meaningless if you do not supply both -A
          and -S.</para>
        </listitem>

        <listitem>
          <para>Handling of load errors</para>

          <para>A later section of this chapter describes SciDB mechanisms for
          handling errors during load. These mechanisms include both a maximum
          error count you supply and a shadow array, which accommodates error
          messages that occur on specific cell locations of the 1-dimensional
          load array. When using loadcsv.py, you use the -e switch to
          establish the maximum error count (per SciDB instance working on the
          load) and -w to give the name of the shadow array.</para>
        </listitem>

        <listitem>
          <para>Location of pipes or intermediate files, and the optional
          persistence of intermediate files</para>

          <para>The program loadcsv.py can distribute the partitioned CSV data
          via files or via pipes. Pipes provide superior performance, but you
          can use files if you want. To request files, use -m. To request that
          such files be retained after the load operation (typically for
          debugging purposes), use -l.</para>

          <para>Likewise, the program can use either pipes or files to
          accommodate the output of the csv2scidb program--the SciDB-readable
          files to be loaded into the destination. By default, the
          dense-load-format result format will be set to a pipe. To request
          files, use -M. To request that such files be retained after the load
          operation (again, typically for debugging), use -L.</para>

          <para>Whether you use pipes or files, you can control the location
          of the output of the splitcsv utility -- the utility that partitions
          the original CSV file. Use the -o switch. The parameter you supply
          with -o indicates the base name of each part of the partitioned
          output. For example, if the command line includes -o '/tmp/base',
          the various files or pipes on the individual server instances would
          be named:</para>

          <para>/tmp/base_0000</para>

          <para>/tmp/base_0001</para>

          <para>etc.</para>
        </listitem>

        <listitem>
          <para>Control of verbose mode, quiet mode, etc. for progress and
          status reporting.</para>

          <para>Use -v for verbose mode, -q for quiet mode, -h for help, and
          -V to show SciDB version information.</para>
        </listitem>
      </itemizedlist>

      <para>This command will load data from aData.csv, which contains one
      header row and three numeric columns, into the existing array aFlat:
      <programlisting>loadcsv.py -n 1 -t NNN
           -a 'aFlat'  
           -i './aData.csv' 
</programlisting></para>

      <para>This command will create the array aFlat and load data from
      aData.csv into it.</para>

      <para><programlisting>loadcsv.py -n 1 -t NNN 
           -a 'aFlat' 
           -s '&lt;row:int64,col:int64,val:int64 null&gt;
               [csvRow=0:*,500000,0]' 
           -i './aData.csv' 
</programlisting></para>

      <para>This command loads data into the existing array aFlat using files
      instead of pipes:</para>

      <para><programlisting>loadcsv.py -n 1 -t NNN
           -a 'aFlat'  
           -i './aData.csv' 
           -o '/home/scidb/aData' 
           -m -M</programlisting></para>

      <para>This command also uses files instead of pipes, and retains those
      files after the load operation:<programlisting>loadcsv.py -n 1 -t NNN
           -a 'aFlat'  
           -i './aData.csv'  
           -o '/home/scidb/aData' 
           -m -l -M -L
</programlisting></para>

      <para>This command loads data into aFlat, and uses a shadow array and a
      maximum error count to handle load errors gracefully.<programlisting>loadcsv.py -n 1 -t NNN
           -a 'aFlat'  
           -i './aData.csv' 
           -o '/home/scidb/aData' 
           -e 10 
           -w 'aFlatshadow'
</programlisting></para>
    </sect2>

    <sect2>
      <title>Rearrange As Necessary</title>

      <para>After you establish the 1-dimensional load array in SciDB, you can
      to translate it into the desired array whose shape accommodates your
      analytical needs: the target multi-dimensional array. Because you are
      using loadcsv.py, you have two choices for accomplishing this step. You
      can use redimension_store after loadcsv.py populates the load array.
      This step is identical to the analogous step described in the section on
      the CSV load technique.</para>

      <para>Alternatively, you can instruct loadcsv.py to transform the
      1-dimensional load array into the target multi-dimensional array. To
      achieve this, use the -A switch and optionally the -S and -X
      switches.</para>

      <para>The following command loads data from the csv file (aData.csv)
      into the existing 1-dimensional load array (aFlat) and rearranges that
      data into the existing target multi-dimensional array (aFinal).</para>

      <para><programlisting>loadcsv.py -n 1 -t NNN
           -a 'aFlat'  
           -i './aData.csv' 
           -o '/home/scidb/aData' 
           -A 'aFinal'
</programlisting></para>

      <para>The following command loads data from the csv file into the load
      array, creates the target multidimensional array, and rearranges the
      data from the load array into the target array. <programlisting>loadcsv.py -n 1 -t NNN
           -a 'aFlat'  
           -i './aData.csv' 
           -o '/home/scidb/aData' 
           -A 'aFinal' 
           -S '&lt;val:int64 null&gt;
               [row=1:*,1000,0,col=1:*,1000,0]'
</programlisting></para>

      <para>The following command loads data from the csv file into the load
      array, establishes a shadow array for the load operations, and
      rearranges the data from the load array into the target array.</para>

      <para><programlisting>loadcsv.py -n 1 -t NNN
           -a 'aFlat'  
           -i './aData.csv' 
           -o '/home/scidb/aData' 
           -A 'aFinal'
           -w 'aFlatShadow</programlisting></para>

      <note>
        <para>The shadow array corresponds to the 1-dimensional load array,
        not the multidimensional target array.</para>
      </note>
    </sect2>
  </sect1>

  <sect1>
    <title>Loading Binary Data</title>

    <para>The binary loading technique starts from a binary file, loads it
    into a 1-dimensional array in SciDB, and rearranges that 1-dimensional
    array into the multidimensional shape you need to support your querying
    and analytics. The following figure summarizes.</para>

    <figure>
      <title/>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentdepth="300" depth="300"
                     fileref="../graphics/binary_load_overview.png" scale="50"
                     valign="top" width="400"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>Obviously, the binary loading technique commends itself to
    situations in which your external application can produce a binary file.
    But if you can control the format that the external application uses to
    produce the data, you might choose to produce a binary file and to use the
    binary loading technique for loading large arrays when you do not want to
    encounter the overhead involved in parsing CSV files and SciDB-formatted
    text files. For example, avoiding this overhead is especially desirable if
    your data includes many variables whose data type is double.</para>

    <sect2>
      <title>Visualize the Target Array</title>

      <para>When using the binary loading technique, visualizing the desired
      multi-dimensional array means the following:</para>

      <itemizedlist>
        <listitem>
          <para>Determine the attributes for the array, including each
          attribute's name, datatype, whether it allows null values, and
          whether it has a default value to be used to replace null
          values.</para>
        </listitem>

        <listitem>
          <para>Determine the dimensions of the array, including each
          dimension's name and datatype.</para>
        </listitem>
      </itemizedlist>

      <para>When using the binary load technique, you can postpone
      contemplating each dimension's chunk size until after you have loaded
      the data into the load array. This lets you use the analyze operator on
      that array to learn some simple statistics about the loaded data that
      can help you choose chunk sizes and chunk overlaps for each dimension of
      the multi-dimensional array you desire.</para>

      <para>For example, suppose you want an array with two dimensions and one
      attribute, like this:</para>

      <figure>
        <title/>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="400" depth="200"
                       fileref="../graphics/desired_array_intensity.png"
                       scale="50" valign="top" width="300"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The dimensions are "exposure" (with values High, Medium, and Low)
      and "elapsedTime" (with values from 0 to 7 seconds). The sole attribute
      is "measuredIntensity." The bottom right cell indicates, for example,
      that seven seconds after low exposure, the measured intensity is 29.
      Note that the desired array includes some null values for the
      measuredIntensity attribute.</para>

      <para>This simple, 24-cell array will be the target array used to
      illustrate steps of the binary load technique.</para>
    </sect2>

    <sect2>
      <title>Prepare the Binary Load File</title>

      <para>A SciDB binary load file represents a 1-dimensional SciDB array.
      The 1-dimensional array is dense; it has no empty cells (although it can
      have null values for nullable attributes). The binary load file
      represents each cell of the 1-dimensional array in turn; within each
      cell, the file represents each attribute in turn.</para>

      <para>The following two figures illustrate. The first figure shows a
      very simple array: 1 dimension, four attributes, and only two cells. The
      figure also shows the AQL statement that created the array, revealing
      which attributes allow null values.</para>

      <figure>
        <title/>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="600" depth="300"
                       fileref="../graphics/binary_load_array.png" scale="50"
                       valign="top" width="400"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The next figure represents the layout of this array within the
      corresponding binary load file.</para>

      <figure>
        <title/>

        <mediaobject>
          <imageobject>
            <imagedata align="center" contentdepth="400" depth="130"
                       fileref="../graphics/binary_load_file_simplified.png"
                       scale="150" valign="top" width="600"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The figure illustrates the following characteristics of a binary
      load file:</para>

      <itemizedlist>
        <listitem>
          <para>Each cell of the array is represented in contiguous bytes.
          (But remember, some programs that create binary files will pad
          certain values so they align on word boundaries. This figure does
          not show such values. You can use the SKIP keyword to skip over such
          padding.)</para>
        </listitem>

        <listitem>
          <para>There are no end-of-cell delimiters. The first byte of the
          representation of the first attribute value of cell N begins
          immediately after the the last byte of the last attribute of cell
          N-1.</para>
        </listitem>

        <listitem>
          <para>A fixed-length data type that allows null values will always
          consume one more byte than the datatype requires, regardless of
          whether the actual value is null or non-null. E.g., an int8 will
          require 2 bytes and an int64 will require 9 bytes. (In the figure,
          see bytes 2-4 or 17-19.)</para>
        </listitem>

        <listitem>
          <para>A fixed-length data type that disallows null values will
          always consume exactly as many bytes as that datatype requires.
          E.g., an int8 will consume 1 byte and an int64 will consume 8 bytes.
          (See byte 1 or 16.)</para>
        </listitem>

        <listitem>
          <para>A string data type that disallows nulls is always preceded by
          four bytes indicating the string length. (See bytes 10-13 or
          26-29.)</para>
        </listitem>

        <listitem>
          <para>A string data type that allows nulls is always preceded by
          five bytes: a null byte indicating whether a value is present and
          four bytes indicating the string length. For values that are null,
          the string length will be zero. (See bytes 5-9 or 20-24.)</para>
        </listitem>

        <listitem>
          <para>The length of a null string is recorded as zero. (See bytes
          5-9.)</para>
        </listitem>

        <listitem>
          <para>If a nullable attribute contains a non-null value, the
          preceding null byte is -1. (See byte 2 or 20.)</para>
        </listitem>

        <listitem>
          <para>If a nullable attribute contains a null value, the preceding
          null byte will contain the missing reason code, which must be
          between 0 and 127. (See byte 5 or 17.)</para>
        </listitem>

        <listitem>
          <para>The file does not contain index values for the dimension of
          the array to be populated by the LOAD command. The command reads the
          file sequentially and creates the cells of the array accordingly.
          The first cell is assigned the first index value of the dimension,
          and each successive cell receives the next index value.</para>
        </listitem>
      </itemizedlist>

      <para>Storage for a given type is assumed to be in the x86_64 endian
      format.</para>

      <para>Each value in the file must conform to a datatype recognized by
      SciDB. This includes native types, types defined in SciDB extensions,
      and user-defined types. For a complete list of the types recognized by
      your installation of SciDB, use the following AQL:</para>

      <programlisting language="test">
--aql SELECT * FROM list('types'); --show-output=no 
</programlisting>
    </sect2>

    <sect2>
       

      <title>Load the Data</title>

       

      <para>After you prepare the file in the SciDB-recognized binary format,
      you are almost ready to load the data into SciDB. But first you must
      create the load array. The array must have one dimension and N
      attributes, where N is the number of variables (attributes and
      dimensions) in the target array. For the simple example about measured
      intensity after exposure, the array you create must have three
      attributes, like this:</para>

       

      <para>
        <programlisting language="test">
--aql CREATE ARRAY intensityFlat 
 &lt; exposure:string, elapsedTime:int64, measuredIntensity:int64 null &gt; 
 [i=0:*,1000000,0]; --show-output=no --show-query=yes 
</programlisting>
      </para>

       

      <para>Within the preceding CREATE ARRAY statement, notice the
      following:</para>

       

      <itemizedlist>
        <listitem>
          <para>The attribute names -- Although the array intensityFlat is
          merely the load array--one that you might even delete after you
          create and populate the target 2-dimensional, 1-attribute array--the
          attribute names matter. You should name the attributes as you expect
          to name the corresponding attribute and dimensions in the array you
          will ultimately create to support your analytics.</para>
        </listitem>

        <listitem>
          <para>The order of attributes -- You should declare the attributes
          in the same left-to-right order as the values that appear in each
          record of the binary file.</para>
        </listitem>

        <listitem>
          <para>The null declaration for the measuredIntensity attribute. This
          is needed because the data includes some null values for that
          attribute.</para>
        </listitem>

        <listitem>
          <para>The dimension name -- The dimension name ("i" in this case) is
          uninteresting. You can use any name, because that dimension does not
          correspond to any variable from your data set and that dimension
          will not appear in any form in the final array you eventually
          create. Remember, the binary load procedure loads the data into a
          1-dimensional array where every variable of your data appears as an
          attribute. These variables are not rearranged into attributes and
          dimensions until the last step of the procedure.</para>
        </listitem>

        <listitem>
          <para>The chunk size (in this case, 1000000) for the dimension --
          Even though you might use the intensityFlat array only briefly and
          delete it after you establish and populate the target array, the
          chunk size of the load array matters because it can affect
          performance of the load and of the next step: the redimension_store.
          The chunk size you choose for the load array has no effect on the
          chunk sizes you will eventually choose for the exposure and
          elapsedTime dimensions of the target array.</para>
        </listitem>

        <listitem>
          <para>The chunk overlap (in this case, 0) for the dimension. For the
          intermediate array that exists only as the target of a load and as
          the source of a subsequent redimension_store, there is no need for
          chunks to overlap at all.</para>
        </listitem>
      </itemizedlist>

       

      <para>For more information about chunk size and overlap, see the section
      on Basic Architecture in Introduction to SciDB.</para>

       

      <para>After you create the load array, you can populate it with data
      using the <code>LOAD</code> statement: <programlisting language="test">
--aql LOAD intensityFlat FROM '../examples/intensity_data.bin'
      AS '(string, 
           int64, 
           int64 null)';  --show-output=no 
</programlisting></para>

       The file path of 

      <replaceable>intensity_data.bin</replaceable>

       is relative to the SciDB server's working directory on the coordinator instance. 

      <para>Notice the format string--the quoted text following the AS
      keyword. The LOAD command uses the format string as a guide for
      interpreting the contents of the binary file. For more information about
      the syntax of the format string, see the Load section in SciDB Operator
      Reference.</para>

       
    </sect2>

    <sect2>
      <title>Rearrange As Necessary</title>

      <para>After you populate the load array, you can use SciDB features to
      translate it into the desired array whose shape accommodates your
      analytical needs. Of course, you should have the basic shape of the
      target array in mind from the outset -- perhaps even before you created
      the binary file.</para>

      <para>There are, however, some characteristics of arrays beyond these
      basics. These include the chunk size and chunk overlap value of each
      dimension. Before you choose values for these parameters, you can use
      the SciDB analyze operator to learn some simple statistics about the
      data in the 1-dimensional array you loaded. Here is the command to
      analyze the array intensityFlat:</para>

      <para><programlisting language="test">
--aql SELECT * FROM analyze(intensityFlat)
</programlisting> Of course, for the simple example presented here, the simple
      statistics reveal little of interest. For large arrays however, the data
      can be illuminating and can influence your decisions about chunk size
      and chunk overlap. For more information about chunk size and overlap,
      see the section on Basic Architecture in Introduction to SciDB. For more
      information about the analyze operator, see the Analyze section in SciDB
      Operator Reference.</para>

      <para>The following command creates the desired 2-dimensional,
      1-attribute array:</para>

      <para><programlisting language="test">
--aql CREATE ARRAY 
            intensity 
           &lt;measuredIntensity:int64 null&gt; 
           [exposure(string)=3,3,0,
            elapsedTime=0:40000,10000,0]; --show-output=no 
</programlisting> The result of that command is an array that can accommodate
      the data about measured intensity after exposure. To populate this array
      with the data, use the following command:</para>

      <para><programlisting language="test">
--aql SELECT * FROM redimension_store(intensityFlat,intensity); --show-output=no
</programlisting> The result of this command is the desired array; you have
      completed the binary load procedure.</para>
    </sect2>

    <sect2>
      <title>Skipping Fields and Field Padding During Binary Load</title>

      <para>During binary load, you can instruct the loader to skip some data
      in the file. This is useful when you want exclude entire fields from the
      load operation, and when you want to skip over some padded bytes that
      have been added to a field by the application that produced the binary
      file.</para>

      <para>For skipping entire fields: From a binary file with N attributes,
      you can load a 1-dimensional SciDB array that has M attributes, where M
      &lt; N. You do this with the SKIP keyword. Compare the following three
      pairs of AQL statements, which create and populate arrays excluding
      zero, one, and two fields of the same load file.</para>

      <para>The first pair of statments includes all fields:</para>

      <para><programlisting language="test">
--afl remove(intensityFlat); --show-query=no --show-output=no
--aql CREATE ARRAY  
       intensityFlat 
         &lt; exposure:string, 
           elapsedTime:int64, 
           measuredIntensity:int64 null &gt;
       [i=0:*,1000000,0]; --show-output=no 

--aql LOAD intensityFlat
     FROM '../examples/intensity_data.bin'
     AS   '(string, 
            int64, 
            int64 null)'; --show-output=no 
</programlisting> The second pair of statements excludes a string
      field:</para>

      <para><programlisting language="test">

--aql CREATE ARRAY intensityFlat_NoExposure 
 &lt; elapsedTime:int64, measuredIntensity:int64 null &gt; 
 [i=0:*,1000000,0]; --show-output=no 
--aql LOAD intensityFlat_NoExposure
     FROM '../examples/intensity_data.bin'
     AS   '(skip, 
            int64, 
            int64 null)'; --show-output=no
</programlisting> The third pair of statements excludes two int64 fields, one
      of which allows null values: <programlisting language="test">
--aql CREATE ARRAY  intensityFlat_NoTime_NoMeasurement 
 &lt; exposure:string &gt; 
 [i=0:*,1000000,0];  --show-output=no

--aql LOAD intensityFlat_NoTime_NoMeasurement
     FROM '../examples/intensity_data.bin'
     AS   '(string, 
            skip(8), 
            skip(8) null)';  --show-output=no
</programlisting> The preceding pairs of AQL statements illustrate the
      following characteristics of the SKIP keyword:</para>

      <itemizedlist>
        <listitem>
          <para>For variable-length fields, you can use the SKIP keyword
          without a number of bytes.</para>
        </listitem>

        <listitem>
          <para>For fixed-length fields, you can use the SKIP keyword with a
          number of bytes in parentheses.</para>
        </listitem>

        <listitem>
          <para>To skip a field that contains null values, use the NULL
          keyword after the SKIP keyword.</para>
        </listitem>
      </itemizedlist>

      <para><note>
          <para>When writing field values into a file, some programming
          languages will always align field values to start on 32-bit word
          boundaries.</para>
        </note></para>
    </sect2>
  </sect1>

  <sect1>
    <title>Transferring Data From One SciDB Installation to Another</title>

    <para>The data-loading technique that transfers array data from one SciDB
    installation to another is called the "opaque" technique, so named because
    the intermediate file format is not user-programmable. The opaque
    technique starts from any array in one SciDB installation, produces an
    external file, and loads that file into another SciDB
    installation--establishing an array that had the same dimensions,
    attributes, dimension indexes, and attribute values as the original array
    from the source installation. The following figure presents an
    overview.</para>

    <figure>
      <title/>

      <mediaobject>
        <imageobject>
          <imagedata align="center" contentdepth="300" depth="300"
                     fileref="../graphics/opaque_load_overview.png" scale="30"
                     valign="top" width="400"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The opaque data-loading technique is recommended in the following
    situations:</para>

    <itemizedlist>
      <listitem>
        <para>The source of the data is an existing SciDB array (rather than a
        CSV file or binary file).</para>
      </listitem>

      <listitem>
        <para>You want to use a simple procedure that requires few commands
        and few intermediate results</para>
      </listitem>

      <listitem>
        <para>You want to avoid the responsibility for ensuring that your load
        file is in the correct format.</para>
      </listitem>

      <listitem>
        <para>The array you are transfering includes a dimension whose
        datatype is not int64.</para>
      </listitem>
    </itemizedlist>

    <sect2>
      <title>Visualize the Desired Array</title>

      <para>When using the opaque loading technique, visualizing the SciDB
      array you want in the destination SciDB installation can be easy because
      the desired array--or something very close to it--already exists in the
      source installation. In the most straighforward case, you can transfer
      the current version of the source array to the destination array without
      modification: the array in the destination installation will match the
      source array in all of the following ways:</para>

      <itemizedlist>
        <listitem>
          <para>Dimensions: Same dimension names, datatypes, upper and lower
          bounds, index values, and the same order of dimensions.</para>
        </listitem>

        <listitem>
          <para>Attributes: Same attribute names and datatypes, and the same
          order of attributes within cells.</para>
        </listitem>

        <listitem>
          <para>Cells: Same cell values.</para>
        </listitem>

        <listitem>
          <para>Chunks: For each dimension, the same values for the chunk size
          and chunk overlap parameters.</para>
        </listitem>
      </itemizedlist>

      <para>Beyond this most straightforward case, there are cases in which
      you make slight adjustments to the array, either in the source
      installation before you create the file in opaque format, or in the
      destination installation when you create the array that will contain the
      data loaded from the file. The next two sections elaborate on these
      cases.</para>

      <para/>
    </sect2>

    <sect2>
      <title>Prepare the File for Opaque Loading</title>

      <para>The opaque loading technique can create a file describing the
      current version of any SciDB array. The following command accomplishes
      that for the array called "intensity."</para>

      <para><programlisting language="test">SAVE 
          intensity
     INTO CURRENT INSTANCE 
          '../examples/intensity_data.opaque'
     AS   'OPAQUE';</programlisting>The keywords <code>CURRENT INSTANCE</code>
      instruct SciDB to create the file in the SciDB working directory on the
      coordinator node. The preceding SAVE statement writes the
      opaque-formatted file in this location on the coordinator instance of
      the source installation:</para>

      <para><replaceable>base-path</replaceable>../examples/intensity_data.bin</para>

      <para>where <replaceable>base-path</replaceable> is the location of your
      SciDB working directory (as defined in the SciDB config.ini
      file).</para>

      <para>The keyword <code>OPAQUE</code> instructs SciDB to create the file
      in opaque format.</para>

      <para>If you want the resulting opaque-formatted file to describe
      something other than the original array--say, a subset of it or an array
      with an additional attribute--you can modify the array accordingly using
      various SciDB operators. There are two methods:</para>

      <itemizedlist>
        <listitem>
          <para>In AQL, you can establish the result array you want, store it,
          and then use the SAVE command on the newly stored array. The AQL
          SAVE syntax does not currently support saving non-stored arrays, so
          you must explicitly store the array you want to SAVE to a load
          file.</para>
        </listitem>

        <listitem>
          <para>In AFL, you can establish the result array you want and use
          that result array as an operand of the SAVE operator. For more
          information, see the SAVE section of the SciDB Operator Reference
          chapter.</para>
        </listitem>
      </itemizedlist>

      <para>After you have saved the file in the working directory on the
      coordinator node, you need to move the file to a location where the
      other SciDB installation can access it when you run the load command
      there.</para>
    </sect2>

    <sect2>
      <title>Load the Data</title>

      <para>Once you have the opaque-formatted file in a location where the
      destination installation of SciDB can access it, you are almost ready to
      load the data. But first you must create an array as the target of the
      load operation. The array you create must match the source array in the
      following regards:</para>

      <itemizedlist>
        <listitem>
          <para>Dimensions: The array in the destination installation must
          have the same number of dimensions as the source array. The
          left-to-right order of the dimensions must have the same datatypes
          as the source array. Note that the names of the dimensions need not
          match the names in the source array.</para>
        </listitem>

        <listitem>
          <para>Attributes: The array in the destination installation must
          have the same number of attributes as the source array. The
          left-to-right order of the attributes must have the same datatypes
          as the source array. Note that the names of the attributes need not
          match the names in the source array.</para>
        </listitem>
      </itemizedlist>

      <para>To ensure that you create a target array that is compatible with
      the to-be-loaded data, you should check the schema of the original array
      on the source installation. The following statement--run on the source
      installation of SciDB--reveals the information you need:</para>

      <para><programlisting language="test">
--schema intensity --show-output=yes
</programlisting></para>

      <para>With that information, you can now create the array in the
      destination installation of SciDB. The following command creates an
      array that is compatible with the data in the opaque-formatted load
      file: <programlisting language="test">
--aql CREATE ARRAY intensityCopy 
         &lt; measuredIntensity:int64 NULL &gt; 
         [ exposure(string)=3,3,0,
           duration=0:40000,10000,0] --show-output=no
</programlisting>Notice that the array differs from the source array in two
      regards that do not compromise the compatibility with the
      opaque-formated load file. The source array is called "intensity" but
      the destination array is called "intensityCopy." A dimension of the
      source array is called "elapsedTime" but the corresponding dimension of
      the destination array is called "duration."</para>

      <para>Now that the destination array exists, you can load the data into
      it:</para>

      <para><programlisting language="test">
--aql LOAD intensityCopy
     FROM CURRENT INSTANCE '../examples/intensity_data.opaque'
     AS 'OPAQUE'; --show-output=no 
</programlisting> The result of this command is the array in the destination
      installation of SciDB. You have completed the opaque loading
      procedure.</para>
    </sect2>
  </sect1>

  <sect1>
    <title>Data with Special Values</title>

    <para>Suppose you have a load file that is missing some values, like this
    file, <code>v4.scidb</code>:</para>

    <programlisting language="test">--shell --show-output=yes --show-query=no --command=cat ../examples/v4.scidb</programlisting>

    <para>The load file <code>v4.scidb</code> has a missing value in the third
    cell. If you create an array and load this data set, SciDB will substitute
    0 for the missing value: <programlisting language="test">--aql CREATE ARRAY v4 &lt;val1:int8,val2:int8&gt;[i=0:3,4,0] --show-output=no
--aql LOAD v4 FROM '../examples/v4.scidb'; --show-output=no</programlisting></para>

    <para>The out-of-the-box default value for each datatype is described in
    the chapter describing SciDB datatypes: SciDB Data Types and Casting.To
    change the default value, that is, the value the SciDB substitutes for the
    missing data, set the DEFAULT attribute option. This code creates an array
    <code>v4_dflt</code> with default attribute value set to 111:</para>

    <programlisting language="test">--aql CREATE ARRAY v4_dflt &lt;val1:int8,val2:int8 default 111&gt;[i=0:3,4,0] --show-output=no
--aql LOAD v4_dflt FROM '../examples/v4.scidb';  --show-output=no</programlisting>

    <para>Load files may also contain null values, such as in this file,
    <code>v4_null.scidb</code>: <programlisting language="test">--shell --show-query=no --show-output=yes --command=cat ../examples/v4_null.scidb</programlisting>
    To preserve null values at load time, add the NULL option to the attribute
    type: <programlisting language="test">--aql CREATE ARRAY v4_null &lt;val1:int8,val2:int8 NULL&gt; [i=0:3,4,0]; --show-output=no 
--aql LOAD v4_null FROM '../examples/v4_null.scidb'; --show-output=no </programlisting></para>
  </sect1>

  <sect1>
     

    <title>Handling Errors During Load</title>

     

    <para>By default, if an error occurs during load, SciDB displays an error
    message to stdout and cancels the operation. Because load is designed to
    work on high volumes of data, the SciDB load facility includes a mechanism
    by which you can keep track of errors while still loading the error-free
    values. This mechanism is known as "shadow arrays."</para>

     

    <para>During a load operation, SciDB can populate two arrays:</para>

     

    <itemizedlist>
      <listitem>
        <para>The load array is populated with data from the load file.</para>
      </listitem>

      <listitem>
        <para>The shadow array is populated with error messages that occurred
        during the load.</para>
      </listitem>
    </itemizedlist>

     

    <para>The shadow array uses the same dimensions and dimension values as
    the load array.</para>

     

    <para>For attributes, things are slightly different. If the load array has
    n attributes, the shadow array has n+1 attributes, as follows:</para>

     

    <itemizedlist>
      <listitem>
        <para>For each attribute in the load array, the shadow array includes
        an identically named attribute. Although the names are identical, the
        datatypes are not. In the shadow array, each of these n attributes is
        a string.</para>
      </listitem>

      <listitem>
        <para>The shadow array includes one additional integer attribute named
        "row_offset." After the load operation, this attribute is populated
        for any cell that contains an error message. The value of row_offset
        describes SciDB's best estimate of the byte of the file on which the
        error occurs.</para>
      </listitem>
    </itemizedlist>

     

    <para>After a load operation that is largely successful but produces a few
    errors, most cells of the shadow array will be empty; such cells
    correspond to cells in the load array that were loaded without error.
    Within any non-empty cell in the shadow array, the row_offset contains an
    integer, and each string attribute is either null (indicating that the
    corresponding value was loaded into the load array without error) or
    populated with a message describing the error.</para>

     

    <para>Note that SciDB will create a shadow array automatically for you if
    you use the <code>SHADOW ARRAY</code> keywords in your LOAD
    statement.</para>

     

    <para>By using shadow arrays, you can achieve a successful load, even if
    the load file contains some imperfections. Of course, if a file is grossly
    deformed or incompatible with the load operation, you probably want SciDB
    to abandon the load operation. In such a case, you can include with the
    load statement a maximum number of errors, after which SciDB should
    abandon the load operation. To specify the maximum number of errors, use
    the <code>ERRORS</code> keyword.</para>

     

    <para>For example, consider the following CREATE ARRAY statement that
    establishes a 1-dimensional array to serve as the load array:</para>

     

    <para><programlisting language="test">
--afl remove(intensityFlat); --show-query=no --show-output=no
--aql CREATE ARRAY  intensityFlat 
 &lt; exposure:string, elapsedTime:int64, measuredIntensity:int64 null &gt;
 [i=0:6,1000000,0]; --show-output=no 
</programlisting>Assume that you want to load into this array the data from
    the following csv file, which contains some errors:</para>

     

    <para/>

     

    <programlisting language="test">--shell --show-query=no --command=cat ../examples/csv-errors.txt</programlisting>

     

    <para>As you compare the CSV file with the CREATE ARRAY statement, notice
    the following:</para>

     

    <itemizedlist>
      <listitem>
        <para>The second row contains an error--a text value in a numeric
        field.</para>
      </listitem>

      <listitem>
        <para>The fifth row contains two errors--text values in numeric
        fields.</para>
      </listitem>

      <listitem>
        <para>The sixth row contains two errors--a null value in the second
        field (whose corresponding attribute in the CREATE ARRAY statement
        prohibits nulls) and a text value in the third field.</para>
      </listitem>

      <listitem>
        <para>The seventh row contains a legitimate null value in the third
        field.</para>
      </listitem>

      <listitem>
        <para>All other rows are unremarkable.</para>
      </listitem>
    </itemizedlist>

     

    <para>The corresponding SciDB-formatted text file--that is, the file that
    results when you run csv2scidb on this CSV file, is shown below:</para>

     

    <programlisting language="test">--shell --command=cat '../examples/int4error.scidb'  --show-output=yes</programlisting>

     

    <para>To load this file into SciDB using a shadow array to keep track of
    load errors, use this AQL statement:</para>

     

    <programlisting language="test">
--aql LOAD intensityFlat
      FROM '../examples/int4error.scidb'
      AS   'text'
      ERRORS 99
      SHADOW ARRAY intensityFlatShadow; 
 --show-output=yes</programlisting>

     In the LOAD statement, notice the following: 

    <itemizedlist>
      <listitem>
        <para>The LOAD statement establishes a limit of 99 errors for the
        load. If the load operation encounters more than 99 errors, SciDB will
        abandon it.</para>
      </listitem>

      <listitem>
        <para>The LOAD statement uses a shadow array named intensityFlatShadow
        to record load errors. If the shadow array you name in the LOAD
        statement does not already exist, SciDB will create it for you. If the
        shadow array you name already exists, you must ensure that the shadow
        array's schema is properly compatible with the schema of the load
        array: string attributes with names identical to the attributes of the
        target array (string or otherwise) plus an int64 attribute named
        row_offset.</para>
      </listitem>
    </itemizedlist>

     

    <para>One result of this LOAD statement is the array intensityFlatShadow.
    To examine its schema definition, use the show operator:</para>

     

    <programlisting language="test">--schema intensityFlatShadow  --show-output=yes</programlisting>

     

    <para>Notice that the shadow array includes one string attribute for every
    attribute (string or otherwise) in the target array. Notice also the
    integer row_offset attribute. Furthermore, notice that the dimension
    declaration--in this case, just the single dimension named i--matches the
    dimension declaration in the load array in all regards: bounds, chunk
    size, and chunk overlap.</para>

     

    <para>Another result of the preceding LOAD statement is the set of
    populated values of the shadow array. To examine these values, use this
    AQL statement:</para>

     

    <programlisting language="test">--aql SELECT * FROM intensityFlatShadow;  --show-output=yes</programlisting>

     

    <para>The data in the intensityFlatShadow array includes three non-empty
    cells, indicating the following:</para>

     

    <itemizedlist>
      <listitem>
        <para>One row (the second) produced an error in the second field. The
        error occurred approximately 35 bytes from the start of the
        file.</para>
      </listitem>

      <listitem>
        <para>One row (the fifth) produced two errors, in the second and third
        fields. The first of these errors occured approximately 94 bytes from
        the start of the file.</para>
      </listitem>

      <listitem>
        <para>One row (the sixth) produced two errors, in the second and third
        fields. The first of these errors occured approximately 110 bytes from
        the start of the file.</para>
      </listitem>

      <listitem>
        <para>All other rows were loaded successfully.</para>
      </listitem>
    </itemizedlist>

     

    <para>And of course, the other result of the LOAD command is data loaded
    into the load array. To examine that data, use this AQL statement:</para>

     

    <programlisting language="test">--aql SELECT * FROM intensityFlat;  --show-output=yes</programlisting>

     

    <para>The data in the intensityFlat array indicates the following:</para>

     

    <itemizedlist>
      <listitem>
        <para>The second row has two correct values (High and 99) in the first
        and third attributes. The second attribute, whose incoming value
        generated an error, has been populated with the default value (0) for
        that field. SciDB inserted the default value because that attribute
        does not allow nulls.</para>
      </listitem>

      <listitem>
        <para>The fifth row has one correct value (Medium) in the first
        attribute. The second attribute has been populated with the default
        value for that attribute. By contrast, the third attribute, which also
        generated an error, has been set to null because that attribute allows
        null values.</para>
      </listitem>

      <listitem>
        <para>The sixth row has one correct value (Medium) in the first
        attribute. The second attribute has been populated with the default
        value for that attribute because that attribute does not alllow nulls.
        By contrast, the third attribute, which also generated an error, has
        been set to null because that attribute allows null values.</para>
      </listitem>

      <listitem>
        <para>All other rows were loaded successfully. This includes the last
        row, whose null value in the third attribute was represented in the
        original CSV file.</para>
      </listitem>
    </itemizedlist>

     

    <para>After a load operation that produced some errors, you can create an
    array that combines the error messages in the shadow array with the
    problematic cells in the load array. For example, the following AQL
    statement accomplishes this with intensityFlat and
    intensityFlatShadow:</para>

     

    <programlisting language="test">--aql SELECT     
        intensityFlat.exposure
                         AS exp, 
        intensityFlatShadow.exposure
                         AS expMSG, 
        intensityFlat.elapsedTime
                         AS elTime, 
        intensityFlatShadow.elapsedTime
                         AS elTimeMSG, 
        intensityFlat.measuredIntensity
                         AS Intensity, 
        intensityFlatShadow.measuredIntensity
                         AS IntensityMSG, 
        row_offset 
     INTO  
        intensityFlatBadCells 
     FROM
        intensityFlat, 
        intensityFlatShadow;
 --show-output=no </programlisting>

     

    <para>You can examine the result of this AQL statement as follows:</para>

     

    <programlisting language="test">--aql SELECT * FROM intensityFlatBadCells; --show-output=yes</programlisting>

     

    <para>The query result shows the usefulness of the array
    intensityFlatBadCells. The array contains one non-empty cell for each
    problematic cell of the load operation. Within the array
    intensityFlatBadCells, the attributes are arranged in consecutive pairs,
    where each pair consists of a value from the load array, and an indication
    of whether that value was successfully loaded. For example, the third
    non-empty cell of intensityFlatBadCells indicates the following:</para>

     

    <itemizedlist>
      <listitem>
        <para>The value in the first attribute of the cell in the load array
        ("Medium") was successfully loaded because the error message
        corresponding to that attribute in the shadow array is null.</para>
      </listitem>

      <listitem>
        <para>The value in the second attribute of the cell in the load array
        was not successfully loaded because the error message is not null. The
        value (0) is the applicable default value for that attribute.</para>
      </listitem>

      <listitem>
        <para>The value in the third attribute of the cell in the load array
        was not successfully loaded because the error message is not null. The
        value itself is null because that attribute of the target array allows
        nulls.</para>
      </listitem>

      <listitem>
        <para>SciDB estimates that the problems loading values for this cell
        begin at or near byte number 110 of the load file.</para>
      </listitem>
    </itemizedlist>

     

    <para>As you can see, an array like intensityFlatBadCells constitutes a
    useful report on the results of a load operation. Whenever you perform a
    load operation using a shadow array, you can combine the shadow array with
    the target array to make an array like intensityFlatBadCells. Thereafter,
    you can use that array to help you investigate the problems that occured
    during the load. How you choose to remedy or otherwise respond to those
    problems depends on the nature of your data and the data-quality policies
    of your organization.</para>

     
  </sect1>

  <!--
 -->

  <!--
  <sect1>
    <title>Parallel Data Loading -NEW</title>

    <para>What's the basics, and when to use it.</para>

    <sect2>
      <title>When to Use Simple Data Loading</title>

      <para>Regardless of the specific data-loading technique you use, the
      general steps for moving data into SciDB are as follows:</para>
    </sect2>

    <sect2>
      <title>Visualize the desired array</title>

      <para>blah blah blah</para>
    </sect2>

    <sect2>
      <title>Prepare the File</title>

      <para>blah blah blah</para>
    </sect2>

    <sect2>
      <title>Load the Data</title>

      <para>blah blah blah</para>
    </sect2>

    <sect2>
      <title>Rearrange As Necessary</title>

      <para>blah blah blah</para>
    </sect2>
  </sect1>-->

  <!-- <sect1>
    <title>Saving Data from a SciDB Array to a File</title>

    <para>You can save all or part of the data that is contained in a SciDB
    array to a file. You can use a <code> <command>SELECT</command> </code>
    statement with the <code> <command>SAVE</command> </code> clause to save
    an entire array. For example, consider the following array
    <code>random_numbers</code>:<programlisting>AQL% <command>CREATE ARRAY</command> random_numbers &lt;val:double&gt;[i=0:99,100,0];
AQL% <command>SELECT</command> * <command>INTO</command> random_numbers 
     <command>FROM</command> build(random_numbers,random());</programlisting>You
    can save the values stored in the array <code>random_numbers</code> to a
    file with the following query:<programlisting>AQL% <command>SAVE</command> random_numbers 
     <command>INTO</command> '/tmp/random_data.txt';</programlisting>This
    statement saves a SciDB-formatted file called
    <code>random_data.txt</code>.</para>

    <para>To save the data to <code>csv</code> format, set the
    <code>iquery</code> output option to <code>csv</code>:<programlisting>% iquery -o csv -q "<command>SAVE</command> random_numbers 
     <command>INTO</command> '/tmp/random_data.csv';"</programlisting><screen>val,val_rand
1,939618095
2,1011655774
3,3620619210
4,2317057332
5,6137260845
6,10771327980
7,4496569336
8,10364290328
9,2309513805
10,1398261690</screen></para>

    <note>
      <para>You will need to enter your iquery statement directly at the
      command line to change the output option to csv. Type <code>exit;</code>
      at the <code>AQL%</code> prompt to stop the current <code>iquery</code>
      session.</para>
    </note>
  </sect1>
-->

  <!--
  <sect1>
  <title>Data Files in Two or More Dimensions</title>
  <para>SciDB data load files must be organized for dimensions according to the schema of the target array. The following examples show how to load data into a 2-dimensional SciDB array.</para>
  <para>Given an array defined as follows: <programlisting>CREATE ARRAY Two_Dim&lt;a: int32, c: char&gt;[I=0:7,4,0, J=0:7,4,0];</programlisting>The external load file must contain chunks corresponding to array chunks. So, for the example 2-dimensional array with the following chunk layout:  <programlisting>C11 C12 
C21 C22</programlisting></para>
  <para>SciDB supports two formats in load files corresponding to dense and sparse data sets. The sparse format is more efficient when a majority of the cells in the array  do not contain any data. </para>
  <sect2>
    <title>Dense Load Format</title>
    <para>The dense load format    <itemizedlist>
        <listitem>
          <para>Data to be loaded is divided up into chunks, with each chunk enclosed within a &apos;[ ]&apos; and separated by a semi-colon. </para>
        </listitem>
        <listitem>
          <para>Each cell contains a comma-separated list of attribute values placed within (). This &apos;dense&apos; representation can denote empty cells using &apos;()&apos;. </para>
        </listitem>
        <listitem>
          <para>Cells within each chunk must appear in left-to-right dimension order (e.g., row-major order for a 2-dimensional array, or generalized appropriately to higher dimensions). </para>
        </listitem>
        <listitem>
          <para>The location of each cell within the chunk is implicit in the order of the load data.</para>
        </listitem>
      </itemizedlist></para>
    <para>A load file for a 2-dimensional dense array looks like this:<programlisting>[
[ (0, &apos;A&apos;), (1, &apos;B&apos;), (2, &apos;C&apos;), (3, &apos;D&apos;)],
[ (8, &apos;I&apos;), (9, &apos;J&apos;), (10, &apos;K&apos;), (11, &apos;L&apos;)],

[ (16, &apos;Q&apos;), (17, &apos;R&apos;), (18, &apos;S&apos;), (19, &apos;T&apos;)],
[ (24, &apos;Y&apos;), (25, &apos;Z&apos;), (26, &apos;A&apos;), (27, &apos;B&apos;)]
];
[
[ (4, &apos;E&apos;), (5, &apos;F&apos;), (6, &apos;G&apos;), (7, &apos;H&apos;)],

[ (12, &apos;M&apos;), (13, &apos;N&apos;), (14, &apos;O&apos;), (15, &apos;P&apos;)],
[ (20, &apos;U&apos;), (21, &apos;V&apos;), (22, &apos;W&apos;), (23, &apos;X&apos;)],
[ (28, &apos;C&apos;), (29, &apos;D&apos;), (30, &apos;E&apos;), (31, &apos;F&apos;)]

];
[
[ (32, &apos;G&apos;), (33, &apos;H&apos;), (34, &apos;I&apos;), (35, &apos;J&apos;)],
[ (40, &apos;O&apos;), (41, &apos;P&apos;), (42, &apos;Q&apos;), (43, &apos;R&apos;)],
[ (48, &apos;W&apos;), (49, &apos;X&apos;), (50, &apos;Y&apos;), (51, &apos;Z&apos;)],

[ (56, &apos;E&apos;), (57, &apos;F&apos;), (58, &apos;G&apos;), (59, &apos;H&apos;)]
];
[
[ (36, &apos;K&apos;), (37, &apos;L&apos;), (38, &apos;M&apos;), (39, &apos;N&apos;)],
[ (44, &apos;S&apos;), (45, &apos;T&apos;), (46, &apos;U&apos;), (47, &apos;V&apos;)],

[ (52, &apos;A&apos;), (53, &apos;B&apos;), (54, &apos;C&apos;), (55, &apos;D&apos;)],
[ (60, &apos;I&apos;), (61, &apos;J&apos;), (62, &apos;K&apos;), (63, &apos;L&apos;)]
] </programlisting></para>
  </sect2>
 
</sect1>


  <sect1>
    <title>Loading Text Data</title>

    <para>The text loading technique starts from a properly formatted text
    file, which the SciDB loader parses and translates directly into a SciDB
    array that can have arbitrarily many dimensions. (Contrast this with the
    CSV technique, in which the loader produces a 1-dimensional array, which
    you then must redimension into the desired multi-dimensional array that
    supports your analytical needs.)</para>

    <para>The text loading technique is recommended in the following
    situations:</para>

    <itemizedlist>
      <listitem>
        <para>When you want to avoid the redimension_store operation that is
        required at the end of the CSV technique.</para>
      </listitem>

      <listitem>
        <para>When you already know the chunk size and chunk overlap for each
        dimension.</para>
      </listitem>

      <listitem>
        <para>When every dimension of the target array-the array that is the
        destination of the load operation-is of type int64.</para>
      </listitem>
    </itemizedlist>

    <sect2>
      <title>Visualize the desired array</title>

      <para>Because the text loading technique creates a multi-dimensional
      array directly from an input file, you should thoroughly understand your
      desired array before you begin the procedure. That means you should know
      enough about your data to make judicious choices for chunk size and
      chunk overlap parameters for each dimension. In fact, you need to know
      which cells will appear in which chunks of the stored array created by
      the load operation. For more information about chunk size and chunk
      overlap, see the section on Basic Architecture in the Introduction to
      SciDB.</para>
    </sect2>

    <sect2>
      <title>Prepare the Load File</title>

      <para>The text loading technique starts with a properly formated ASCII
      file. Of course, the file must express the values of variables
      (dimensions and attributes) for each non-empty cell in the array.
      However, there are other formatting requirements:</para>

      <itemizedlist>
        <listitem>
          <para>The format must accommodate multi-dimensional arrays.</para>
        </listitem>

        <listitem>
          <para>The format must express chunk boundaries. That is, the format
          must include information about which cells will appear in which
          chunks of the stored SciDB array.</para>
        </listitem>
      </itemizedlist>

      <para>Consequently, the formatted files are more elaborate than those
      produced by the <code>csv2scidb</code> shell command used during the CSV
      technique.</para>

      <para>The file format has two flavors: one is optimized for sparse
      arrays, the other is for dense arrays.</para>

      <para>In the text load format, data is listed by chunks. Chunks are
      delimited by two square brackets. There are semicolons between chunks.
      <programlisting>[[<replaceable>chunk1</replaceable>]];
[[<replaceable>chunk2</replaceable>]];</programlisting>Within each chunk, the
      data is organized as a list of cells. Each cell includes the coordinate
      indices of the cell in curly braces and the attributes of the cell
      (separated by commas) in parentheses.<programlisting>[[{<replaceable>index1</replaceable>,<replaceable>index2</replaceable>,...} (<replaceable>attribute1</replaceable>,<replaceable>attribute2</replaceable>,...), ... 
  {<replaceable>indexm</replaceable>,<replaceable>indexn</replaceable>} (<replaceable>attribute1m</replaceable>,<replaceable>attribute2n</replaceable>)]];</programlisting></para>

      <para>For example, a load file for a diagonal 2-D array with two chunks
      looks like this:<programlisting>[[
{0,0}(0,'A'),{1,1}(1,'B'),{2,2}(2,'C'),{3,3}(3,'D')
]];
[[
{4,4}(6,'G'),{5,5}(7,'P'),{6,6}(9,'H')
]]</programlisting>This data is stored like this:<informalfigure>
          <mediaobject>
            <imageobject>
              <imagedata fileref="../graphics/sparse_load.png" scale="30"/>
            </imageobject>
          </mediaobject>
        </informalfigure></para>

      <tip>
        <para>In addition to storing the data, <command> <code>LOAD</code>
        </command> operator returns the data back to the client (or next
        operator in the query). When the data set is very large, you may want
        to suppress the query output. The <code>iquery</code> executable that
        accompanies SciDB includes the <code>-n</code> option for this
        purpose. See "Getting Started with SciDB Development" for how to use
        <code>iquery</code>.</para>
      </tip>
    </sect2>

    <sect2>
      <title>Load the Data</title>

      <para>blah blah blah</para>
    </sect2>

    <sect2>
      <title>Rearrange As Necessary</title>

      <para>One of the benefits of the text loading technique is that the LOAD
      statement produces a multi-dimensional array that supports your querying
      and analytical needs. You do not need to rearrange the array produced by
      the LOAD command; you have completed the text load procedure.</para>
    </sect2>
  </sect1>

  <sect1>
    <title>Sparse Data Loading - OLD</title>

    <sect2>
      <title>Sparse Load Chunks</title>

      <para>Consider a load file like this:<programlisting>[[
{0,0} (11),
{1,0} (21),
{0,1} (12)
]];
[[
{0,2} (13)
]];
[[
{2,0} (31),
{3,0} (41),
{2,1} (32),
{3,1} (42)
]];
[[
{2,2} (33),
{3,3} (44)
]];
[[
{7,0} (81),
{6,1} (72),
{7,1} (82)
]];
[[
{6,2} (73),
{7,2} (83),
{7,3} (84)
]];
[[
{8,0} (91)
]];
[[
{8,2} (93),
{8,3} (94)
]]</programlisting>The chunk distribution in the load file requires that the
      array have chunks of size 2 in the first dimension and chunks of size 2
      in the second dimension. The array schema for this load file
      is:<programlisting>&lt;attribute:int16&gt;[x=0:8,2,0,y=0:3,2,0];</programlisting></para>
    </sect2>
  </sect1>
-->
</chapter>
